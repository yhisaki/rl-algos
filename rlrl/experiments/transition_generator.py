import collections
import logging
from collections.abc import Iterator
from typing import Any, Callable, Optional, Tuple, Union

import numpy as np
from gym import Env
from gym.vector.vector_env import VectorEnv

from rlrl.utils import clear_if_maxlen_is_none, mean_or_nan
from rlrl.wrappers import SingleAsVectorEnv


class TransitionGenerator(Iterator):
    """Iterate over the Markov decision process generated by the gym.

    Examples:
    >>> env = gym.make("Swimmer-v3")
    >>> def random_actor(state):
    ...     return env.action_space.sample()
    >>> interactions = TransitionGenerator(env, random_actor, max_step=10000)
    >>> for step, state, next_state, action, reward, done in interactions:
    ...     ...
    """

    def __init__(
        self,
        environment: Union[Env, VectorEnv],
        actor: Callable[[Any], Any],
        max_step: Optional[int] = None,
        max_episode: Optional[int] = None,
        calc_stats: bool = True,
        step_stats_window: int = None,
        reward_sum_stats_window: int = None,
        logger: logging.Logger = logging.getLogger(__name__),
    ) -> None:
        """[summary]

        Args:
            environment (gym.Env): OpenAI Gym Environment
            actor (Callable[[Any], Any]): it's takes current state and returns action
            max_step (Optional[int], optional): [description]. Defaults to None.
            max_episode (Optional[int], optional): [description]. Defaults to None.
            record_reward_sum (bool, optional): [description]. Defaults to False.
        """
        super().__init__()
        self.env = environment
        self.actor = actor
        self.max_step = max_step
        self.max_episode = max_episode

        assert (max_step is None) ^ (
            max_episode is None
        ), "Either max_episode or max_step must be set to a value."

        if not isinstance(self.env, VectorEnv):
            self.env = SingleAsVectorEnv(self.env)

        self.state = self.env.reset()

        self.num_envs: int = self.env.num_envs

        self.total_step = np.zeros(self.num_envs, np.uint)
        self.total_episode = np.zeros(self.num_envs, np.uint)

        self.episode_step = np.zeros(self.num_envs, np.uint)
        self.episode_reward = np.zeros(self.num_envs, np.float32)  # reward sum per episode

        self.done = np.zeros(self.num_envs, np.bool_)
        self.logger = logger

        self.calc_stats = calc_stats
        if calc_stats:
            self.step_record = collections.deque(maxlen=step_stats_window)
            self.reward_sum_record = collections.deque(maxlen=reward_sum_stats_window)

    def is_finish(self) -> bool:
        if self.max_step is not None:
            return self.max_step <= self.total_step.sum()
        elif self.max_episode is not None:
            return self.max_episode <= self.total_episode.sum()

    def __iter__(self):
        return self

    def __next__(self) -> Tuple[np.ndarray, ...]:
        self.episode_reward *= np.invert(self.done)
        self.episode_step *= np.invert(self.done)

        if self.is_finish():
            raise StopIteration()

        state = np.copy(self.state)
        action = self.actor(state)
        next_state, reward, self.done, info = self.env.step(action)

        self.state = np.copy(next_state)

        self.total_episode += self.done
        self.total_step += np.ones_like(self.total_episode)

        self.episode_reward += reward
        self.episode_step += np.ones_like(self.episode_step)

        if self.done.any():
            done_env_index = np.where(self.done)[0]
            for idx in done_env_index:
                self.reward_sum_record.append(self.episode_reward[idx])
                self.step_record.append(self.episode_step[idx])
                next_state[idx] = info[idx]["terminal_observation"]
                self.logger.info(
                    f"env : {idx}, "
                    f"total_step = {self.total_step[idx]}, "
                    f"reward = {self.episode_reward[idx]}, "
                    f"step = {self.episode_step[idx]}"
                )

        return self.episode_step, state, next_state, action, reward, self.done

    def get_statistics(self) -> dict:
        if self.calc_stats:
            stats = {
                "average_reward_sum": mean_or_nan(self.reward_sum_record),
                "average_step": mean_or_nan(self.step_record),
            }
            clear_if_maxlen_is_none(self.reward_sum_record, self.step_record)
            return stats
        else:
            self.logger.warning("get_statistics() is called even though the calc_stats is False.")
