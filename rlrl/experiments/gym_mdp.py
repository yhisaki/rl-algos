import logging
from collections.abc import Iterator
from typing import Any, Callable, Optional, Tuple, Union

import numpy as np
from gym import Env
from gym.vector.vector_env import VectorEnv


class GymMDP(Iterator):
    """Iterate over the Markov decision process generated by the gym.

    Examples:
    >>> env = gym.make("Swimmer-v2")
    >>> def random_actor(state):
    ...     return env.action_space.sample()
    >>> interactions = GymMDP(env, random_actor, max_step=10000)
    >>> for step, state, next_state, action, reward, done in interactions:
    ...     ...
    """

    def __init__(
        self,
        environment: Union[Env, VectorEnv],
        actor: Callable[[Any], Any],
        max_step: Optional[int] = None,
        max_episode: Optional[int] = None,
        logger: logging.Logger = logging.getLogger(__name__),
    ) -> None:
        """[summary]

        Args:
            environment (gym.Env): OpenAI Gym Environment
            actor (Callable[[Any], Any]): it's takes current state and returns action
            max_step (Optional[int], optional): [description]. Defaults to None.
            max_episode (Optional[int], optional): [description]. Defaults to None.
            record_reward_sum (bool, optional): [description]. Defaults to False.
        """
        super().__init__()
        self.env = environment
        self.actor = actor
        self.max_step = max_step
        self.max_episode = max_episode

        assert (max_step is None) ^ (
            max_episode is None
        ), "Either max_episode or max_step must be set to a value."

        self.state = self.env.reset()

        if isinstance(self.env, VectorEnv):
            self.num_envs: int = self.env.num_envs
            self.is_vec_env = True
        else:
            self.num_envs = 1
            self.is_vec_env = False

        self.__total_step = np.zeros(self.num_envs, np.uint)
        self.__total_episode = np.zeros(self.num_envs, np.uint)

        self.__episode_step = np.zeros(self.num_envs, np.uint)
        self.__episode_reward = np.zeros(self.num_envs, np.float32)  # reward sum per episode

        self.__done = np.zeros(self.num_envs, np.bool_)
        self.logger = logger

    def is_finish(self) -> bool:
        if self.max_step is not None:
            return self.max_step <= self.__total_step.sum()
        elif self.max_episode is not None:
            return self.max_episode <= self.__total_episode.sum()

    @property
    def done(self):
        if self.is_vec_env:
            return self.__done
        else:
            return self.__done[0]

    @done.setter
    def done(self, val):
        if self.is_vec_env:
            self.__done = val
        else:
            self.__done[0] = val

    @property
    def episode_step(self):
        if self.is_vec_env:
            return self.__episode_step
        else:
            return self.__episode_step[0]

    @property
    def episode_reward(self):
        if self.is_vec_env:
            return self.__episode_reward
        else:
            return self.__episode_reward[0]

    @property
    def total_step(self):
        if self.is_vec_env:
            return self.__total_step
        else:
            return self.__total_step[0]

    @property
    def total_episode(self):
        if self.is_vec_env:
            return self.__total_episode
        else:
            return self.__total_episode[0]

    def __next__(self) -> Tuple[np.ndarray, ...]:
        if not (self.is_vec_env) and self.__done:
            self.state = self.env.reset()

        self.__episode_reward *= np.invert(self.__done)
        self.__episode_step *= np.invert(self.__done)

        if self.is_finish():
            raise StopIteration()

        state = self.state
        action = self.actor(state)
        self.state, reward, self.done, _ = self.env.step(action)

        self.__total_episode += self.__done
        self.__total_step += np.ones_like(self.__total_episode)

        self.__episode_reward += reward
        self.__episode_step += np.ones_like(self.__episode_step)

        if self.__done.any():
            done_env_index = np.where(self.__done)[0]
            for idx in done_env_index:
                self.logger.info(
                    f"env : {idx}, "
                    f"total_step = {self.__total_step[idx]}, "
                    f"reward = {self.__episode_reward[idx]}, "
                    f"step = {self.__episode_step[idx]}"
                )

        return self.episode_step, state, self.state, action, reward, self.done
