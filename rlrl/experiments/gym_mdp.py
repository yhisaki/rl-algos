import logging
from collections.abc import Iterator
from typing import Any, Callable, Optional, Tuple, Union

import numpy as np
from gym import Env
from gym.vector.vector_env import VectorEnv
from rlrl.wrappers import SingleAsVectorEnv


class GymMDP(Iterator):
    """Iterate over the Markov decision process generated by the gym.

    Examples:
    >>> env = gym.make("Swimmer-v2")
    >>> def random_actor(state):
    ...     return env.action_space.sample()
    >>> interactions = GymMDP(env, random_actor, max_step=10000)
    >>> for step, state, next_state, action, reward, done in interactions:
    ...     ...
    """

    def __init__(
        self,
        environment: Union[Env, VectorEnv],
        actor: Callable[[Any], Any],
        max_step: Optional[int] = None,
        max_episode: Optional[int] = None,
        logger: logging.Logger = logging.getLogger(__name__),
    ) -> None:
        """[summary]

        Args:
            environment (gym.Env): OpenAI Gym Environment
            actor (Callable[[Any], Any]): it's takes current state and returns action
            max_step (Optional[int], optional): [description]. Defaults to None.
            max_episode (Optional[int], optional): [description]. Defaults to None.
            record_reward_sum (bool, optional): [description]. Defaults to False.
        """
        super().__init__()
        self.env = environment
        self.actor = actor
        self.max_step = max_step
        self.max_episode = max_episode

        assert (max_step is None) ^ (
            max_episode is None
        ), "Either max_episode or max_step must be set to a value."

        if not isinstance(self.env, VectorEnv):
            self.env = SingleAsVectorEnv(self.env)
            # dummy_env = _make()
            # setattr(self.env, "spec", dummy_env.spec)

        self.state = self.env.reset()

        self.num_envs: int = self.env.num_envs

        self.total_step = np.zeros(self.num_envs, np.uint)
        self.total_episode = np.zeros(self.num_envs, np.uint)

        self.episode_step = np.zeros(self.num_envs, np.uint)
        self.episode_reward = np.zeros(self.num_envs, np.float32)  # reward sum per episode

        self.done = np.zeros(self.num_envs, np.bool_)
        self.logger = logger

    def is_finish(self) -> bool:
        if self.max_step is not None:
            return self.max_step <= self.total_step.sum()
        elif self.max_episode is not None:
            return self.max_episode <= self.total_episode.sum()

    def __iter__(self):
        return self

    def __next__(self) -> Tuple[np.ndarray, ...]:
        self.episode_reward *= np.invert(self.done)
        self.episode_step *= np.invert(self.done)

        if self.is_finish():
            raise StopIteration()

        state = self.state
        action = self.actor(state)
        self.state, reward, self.done, _ = self.env.step(action)

        self.total_episode += self.done
        self.total_step += np.ones_like(self.total_episode)

        self.episode_reward += reward
        self.episode_step += np.ones_like(self.episode_step)

        if self.done.any():
            done_env_index = np.where(self.done)[0]
            for idx in done_env_index:
                self.logger.info(
                    f"env : {idx}, "
                    f"total_step = {self.total_step[idx]}, "
                    f"reward = {self.episode_reward[idx]}, "
                    f"step = {self.episode_step[idx]}"
                )

        return self.episode_step, state, self.state, action, reward, self.done

    def get_statics(self):
        return {}
