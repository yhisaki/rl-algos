from collections.abc import Iterator
from typing import Any, Callable, Optional

import gym
import numpy as np


class GymMDP(Iterator):
    """Iterate over the Markov decision process generated by the gym.

    Examples:
    >>> env = gym.make("Swimmer-v2")
    >>> def random_actor(state):
    ...     return env.action_space.sample()
    >>> interactions = GymMDP(env, random_actor, max_step=10000)
    >>> for step, state, next_state, action, reward, done in interactions:
    ...     ...
    """

    def __init__(
        self,
        environment: gym.Env,
        actor: Callable[[Any], Any],
        max_step: Optional[int] = None,
        max_episode: Optional[int] = None,
    ) -> None:
        """[summary]

        Args:
            environment (gym.Env): OpenAI Gym Environment
            actor (Callable[[Any], Any]): it's takes current state and returns action
            max_step (Optional[int], optional): [description]. Defaults to None.
            max_episode (Optional[int], optional): [description]. Defaults to None.
            record_reward_sum (bool, optional): [description]. Defaults to False.
        """
        super().__init__()
        self.env = environment
        self.actor = actor
        self.max_step = max_step
        self.max_episode = max_episode
        assert (max_step is None) ^ (max_episode is None)

        self.total_step = 0
        self.total_episode = 0

        self.state = self.env.reset()

        if isinstance(self.env, gym.vector.vector_env.VectorEnv):
            self.num_envs = self.env.num_envs
            self.is_vec_env = True
            self.episode_step = np.zeros(self.num_envs, np.uint)
            self.done = np.ones(self.num_envs, np.bool_)
            self.reward_sum = np.zeros(self.num_envs, np.float32)
        else:
            self.num_envs = 1
            self.is_vec_env = False
            self.episode_step = 0
            self.done = False
            self.reward_sum = 0

    def is_finish(self) -> bool:
        if self.max_step is not None:
            return self.max_step <= self.total_step
        elif self.max_episode is not None:
            return self.max_episode <= self.total_episode

    def __next__(self):
        if self.is_vec_env and any(self.done):
            self.reward_sum *= np.invert(self.done)
            self.episode_step *= np.invert(self.done)
        elif not (self.is_vec_env) and self.done:
            self.state = self.env.reset()
            self.done = False
            self.episode_step = 0
            self.reward_sum = 0

        if self.is_finish():
            raise StopIteration()

        state = self.state
        action = self.actor(state)
        self.state, reward, self.done, _ = self.env.step(action)

        if self.is_vec_env and any(self.done):
            self.total_episode += np.count_nonzero(self.done)
        elif not (self.is_vec_env) and self.done:
            self.total_episode += 1

        self.episode_step += np.ones_like(self.episode_step) if self.is_vec_env else 1
        self.total_step += self.num_envs
        self.reward_sum += reward
        return self.episode_step, state, self.state, action, reward, self.done
